{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Asmaa Tbaeen is a Palestinian journalist and human rights activist. She is a prominent figure in the Palestinian Authority and is the founder and director of the Palestinian Freedom Center, which is dedicated to advocating for the rights of Palestinians and promoting peace and dialogue.\n"
     ]
    }
   ],
   "source": [
    "from langchain import PromptTemplate\n",
    "from langchain import OpenAI\n",
    "template = \"\"\"Question: {question}\n",
    "\n",
    "Answer: \"\"\"\n",
    "prompt = PromptTemplate(\n",
    "        template=template,\n",
    "    input_variables=['question']\n",
    ")\n",
    "\n",
    "# user question\n",
    "question = \"who is Asmaa Tbaeen?\"\n",
    "\n",
    "\n",
    "import os\n",
    "OPENAI_API_KEY = \"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\"\n",
    "davinci = OpenAI(model_name='text-davinci-003')\n",
    "from langchain.llms import OpenAI\n",
    "from langchain import HuggingFaceHub, LLMChain\n",
    "llm_chain = LLMChain(\n",
    "    prompt=prompt,\n",
    "    llm=davinci\n",
    ")\n",
    "\n",
    "print(llm_chain.run(question))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LLMResult(generations=[[Generation(text=' The Green Bay Packers won the Super Bowl in the 2010 season.', generation_info={'finish_reason': 'stop', 'logprobs': None})], [Generation(text=' 193.04 centimeters', generation_info={'finish_reason': 'stop', 'logprobs': None})], [Generation(text=' Eugene Cernan was the 12th person on the moon. He was the last person to have ever walked on the moon as part of the Apollo 17 mission in 1972.', generation_info={'finish_reason': 'stop', 'logprobs': None})], [Generation(text=' A blade of grass does not have any eyes.', generation_info={'finish_reason': 'stop', 'logprobs': None})]], llm_output={'token_usage': {'prompt_tokens': 75, 'completion_tokens': 62, 'total_tokens': 137}, 'model_name': 'text-davinci-003'})"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Asking Multiple Questions\n",
    "qs = [\n",
    "    {'question': \"Which NFL team won the Super Bowl in the 2010 season?\"},\n",
    "    {'question': \"If I am 6 ft 4 inches, how tall am I in centimeters?\"},\n",
    "    {'question': \"Who was the 12th person on the moon?\"},\n",
    "    {'question': \"How many eyes does a blade of grass have?\"}\n",
    "]\n",
    "res = llm_chain.generate(qs)\n",
    "res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### After adding our own source"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "A blade of grass does not have any eyes.\n",
      "\n",
      "The conclusion of the CDMX document is that the main target for Rumbo should be level D, as it is the predominant socioeconomic status in the Metropolitan Area and in which people have a high probability of using public transport on a daily basis. The majority of the population in the Metropolitan Area earns less than MXN $15,000 monthly (which means less than USD $25 a day), and the predominant income bracket is MXN $5,000 monthly  to $10,000 monthly (which means USD $8 to USD $16 a day). The areas where fewer people live in poverty are located in the centre and the west of CDMX, while the areas where between 20% to 40% of the population live in poverty are in the south of the city and in some neighbourhoods of the outskirts. About 51% of the population in the Metropolitan Area uses public transport (essentially the informal at 76%, followed by the Metro with 30%), although a high percentage of users use two forms of transport to reach their destination (more than 65%).\n",
      "\n",
      "It is not possible to answer this question without prior knowledge.\n",
      "\n",
      "The reviews for the app mx.mirumbo.rumbo include feedback from users lima, merlin, nio, and dreamqlteue. The reviews are generally positive, with ratings of 5 out of 5 stars. The users have praised the app for its features and ease of use.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    " \n",
    "from langchain import OpenAI\n",
    "\n",
    "from llama_index import (GPTVectorStoreIndex,ResponseSynthesizer,LLMPredictor,PromptHelper,SimpleDirectoryReader)\n",
    "from llama_index.retrievers import VectorIndexRetriever\n",
    "from llama_index.query_engine import RetrieverQueryEngine\n",
    "from llama_index.indices.postprocessor import SimilarityPostprocessor\n",
    " \n",
    "llm_predictor = LLMPredictor(llm=OpenAI(temperature=0, model_name=\"text-davinci-003\"))\n",
    "\n",
    "# Configure prompt parameters and initialise helper\n",
    "max_input_size = 4096\n",
    "num_output = 256\n",
    "max_chunk_overlap = 20\n",
    "\n",
    "prompt_helper = PromptHelper(max_input_size, num_output, max_chunk_overlap)\n",
    "\n",
    "# Load documents from the 'data' directory\n",
    "documents = SimpleDirectoryReader('data_doc').load_data()\n",
    "index = GPTVectorStoreIndex.from_documents(documents)\n",
    "\n",
    "# configure retriever\n",
    "retriever = VectorIndexRetriever(\n",
    "    index=index, \n",
    "    similarity_top_k=2,\n",
    ")\n",
    "\n",
    "# configure response synthesizer\n",
    "response_synthesizer = ResponseSynthesizer.from_args(\n",
    "    node_postprocessors=[\n",
    "        SimilarityPostprocessor(similarity_cutoff=0.7)\n",
    "    ]\n",
    ")\n",
    "\n",
    "# assemble query engine\n",
    "query_engine = RetrieverQueryEngine(\n",
    "    retriever=retriever,\n",
    "    response_synthesizer=response_synthesizer,\n",
    ")\n",
    "\n",
    " \n",
    "query = [\"How many eyes does a blade of grass have?\",\"in short: what is the conclusion of CDMX document?\",\"what is the best review ?\", \"summarize all reviews\"]\n",
    "# query\n",
    "for qu in query:\n",
    "    response = query_engine.query(qu)\n",
    "    print(response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using embedded DuckDB with persistence: data will be stored in: .\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import openai\n",
    "import asyncio\n",
    "from datetime import datetime\n",
    "\n",
    "import uvicorn\n",
    "from fastapi import FastAPI, Body\n",
    "from fastapi.middleware.cors import CORSMiddleware\n",
    "from fastapi.responses import RedirectResponse\n",
    "from fastapi import Request\n",
    "\n",
    "from threading import Thread\n",
    "from apscheduler.schedulers.asyncio import AsyncIOScheduler\n",
    "from pydantic import BaseModel\n",
    "\n",
    "from slack_bolt.adapter.socket_mode import SocketModeHandler\n",
    "from slack_sdk import WebClient\n",
    "from slack_bolt import App\n",
    "\n",
    "from langchain.llms import OpenAI\n",
    "from langchain.document_loaders import PyPDFDirectoryLoader\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.chains import ConversationalRetrievalChain\n",
    "from llama_index import SimpleDirectoryReader,GPTVectorStoreIndex\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "  \n",
    "\n",
    "# Set the OPENAI_API_KEY environment variable, for future calls\n",
    "os.environ['OPENAI_API_KEY'] = OPENAI_API_KEY\n",
    "\n",
    "# Set up FastAPI\n",
    "fast_api_app = FastAPI(\n",
    "    title=\"ChatGPTWimt\",\n",
    "    description=\"Application for querying information about our documents, or redirecting to ChatGPT\",\n",
    ")\n",
    "fast_api_app.add_middleware(\n",
    "    CORSMiddleware,\n",
    "    allow_origins=[\"*\"],\n",
    "    allow_methods=[\"*\"],\n",
    "    allow_headers=[\"*\"],\n",
    ")\n",
    "\n",
    "@fast_api_app.get(\"/\", include_in_schema=False)\n",
    "async def docs_redirect():\n",
    "    return RedirectResponse(url=\"/docs\")\n",
    "\n",
    "\n",
    "@fast_api_app.get(\"/health\")\n",
    "async def root():\n",
    "    return {\"status\": \"healthy\"}\n",
    "\n",
    "# Event API & Web API\n",
    "app = App(token=SLACK_BOT_TOKEN) \n",
    "client = WebClient(SLACK_BOT_TOKEN)\n",
    "\n",
    "# load documents from materials directory\n",
    "loader = PyPDFDirectoryLoader(\"materials\")\n",
    "documents = loader.load()\n",
    "\n",
    "# split the documents into chunks\n",
    "text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)\n",
    "texts = text_splitter.split_documents(documents)\n",
    "\n",
    "# # select which embeddings we want to use\n",
    "embeddings = OpenAIEmbeddings()\n",
    "\n",
    "# # create the vectorestore to use as the index\n",
    "db = Chroma.from_documents(texts, embeddings, persist_directory=\".\")\n",
    "db.persist()\n",
    "retriever = db.as_retriever(search_type=\"similarity\", search_kwargs={\"k\":2})\n",
    "memory = ConversationBufferMemory(memory_key=\"chat_history\", return_messages=True)\n",
    "\n",
    "# # create a chain to answer questions \n",
    "qa = ConversationalRetrievalChain.from_llm(OpenAI(temperature=0.9, model_name=\"text-davinci-003\"), retriever, memory=memory, chain_type='map_reduce')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "question = \"in short: what is the conclusion of CDMX document?\"\n",
    "response = qa({\"question\": question})\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'question': 'in short: what is the conclusion of CDMX document?',\n",
       " 'chat_history': [HumanMessage(content='in short: what is the conclusion of CDMX document?', additional_kwargs={}, example=False),\n",
       "  AIMessage(content=' The majority of public transport users in the Metropolitan Area of CDMX belong to the socioeconomic levels D or D+, with the most vulnerable groups being those with the least mobility and most reliance on public transport, as transportation costs take up a higher percentage of their income.', additional_kwargs={}, example=False)],\n",
       " 'answer': ' The majority of public transport users in the Metropolitan Area of CDMX belong to the socioeconomic levels D or D+, with the most vulnerable groups being those with the least mobility and most reliance on public transport, as transportation costs take up a higher percentage of their income.'}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
